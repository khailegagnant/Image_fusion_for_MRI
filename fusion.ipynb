{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr  2 18:53:25 2025       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 4000     On   | 00000000:12:00.0 Off |                  N/A |\n",
      "| 30%   45C    P8     9W / 125W |      1MiB /  7982MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro RTX 4000     On   | 00000000:86:00.0 Off |                  N/A |\n",
      "| 30%   44C    P8    10W / 125W |      1MiB /  7982MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 3012384            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "609"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import nibabel as nib\n",
    "import cv2\n",
    "from tensorflow.keras import backend as K\n",
    "import gc\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 phases from patient101\n",
      "✅ Loaded 2 phases from patient102\n",
      "✅ Loaded 2 phases from patient103\n",
      "✅ Loaded 2 phases from patient104\n",
      "✅ Loaded 2 phases from patient105\n",
      "✅ Loaded 2 phases from patient106\n",
      "✅ Loaded 2 phases from patient107\n",
      "✅ Loaded 2 phases from patient108\n",
      "✅ Loaded 2 phases from patient109\n",
      "✅ Loaded 2 phases from patient110\n",
      "✅ Loaded 2 phases from patient111\n",
      "✅ Loaded 2 phases from patient112\n",
      "✅ Loaded 2 phases from patient113\n",
      "✅ Loaded 2 phases from patient114\n",
      "✅ Loaded 2 phases from patient115\n",
      "✅ Loaded 2 phases from patient116\n",
      "✅ Loaded 2 phases from patient117\n",
      "✅ Loaded 2 phases from patient118\n",
      "✅ Loaded 2 phases from patient119\n",
      "✅ Loaded 2 phases from patient120\n",
      "✅ Loaded 2 phases from patient121\n",
      "✅ Loaded 2 phases from patient122\n",
      "✅ Loaded 2 phases from patient123\n",
      "✅ Loaded 2 phases from patient124\n",
      "✅ Loaded 2 phases from patient125\n",
      "✅ Loaded 2 phases from patient126\n",
      "✅ Loaded 2 phases from patient127\n",
      "✅ Loaded 2 phases from patient128\n",
      "✅ Loaded 2 phases from patient129\n",
      "✅ Loaded 2 phases from patient130\n",
      "✅ Loaded 2 phases from patient131\n",
      "✅ Loaded 2 phases from patient132\n",
      "✅ Loaded 2 phases from patient133\n",
      "✅ Loaded 2 phases from patient134\n",
      "✅ Loaded 2 phases from patient135\n",
      "✅ Loaded 2 phases from patient136\n",
      "✅ Loaded 2 phases from patient137\n",
      "✅ Loaded 2 phases from patient138\n",
      "✅ Loaded 2 phases from patient139\n",
      "✅ Loaded 2 phases from patient140\n",
      "✅ Loaded 2 phases from patient141\n",
      "✅ Loaded 2 phases from patient142\n",
      "✅ Loaded 2 phases from patient143\n",
      "✅ Loaded 2 phases from patient144\n",
      "✅ Loaded 2 phases from patient145\n",
      "✅ Loaded 2 phases from patient146\n",
      "✅ Loaded 2 phases from patient147\n",
      "✅ Loaded 2 phases from patient148\n",
      "✅ Loaded 2 phases from patient149\n",
      "✅ Loaded 2 phases from patient150\n"
     ]
    }
   ],
   "source": [
    "def resize_slice(slice, target_shape):\n",
    "    return cv2.resize(slice, target_shape, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "def load_patient_slices(patient_folder, num_classes=4):\n",
    "    images, masks = [], []\n",
    "    patient_data = []\n",
    "\n",
    "    for phase in [\"ED\", \"ES\"]:\n",
    "        image_path = os.path.join(patient_folder, f\"patient{patient_folder[-3:]}_{phase}_processed.nii.gz\")\n",
    "        mask_path = os.path.join(patient_folder, f\"patient{patient_folder[-3:]}_{phase}_gt_processed.nii.gz\")\n",
    "\n",
    "        if not os.path.exists(image_path) or not os.path.exists(mask_path):\n",
    "            print(f\"⚠️ Missing file: {image_path} or {mask_path}\")\n",
    "            continue  \n",
    "\n",
    "        image_nii = nib.load(image_path)\n",
    "        mask_nii = nib.load(mask_path)\n",
    "\n",
    "        image_array = image_nii.get_fdata()\n",
    "        mask_array = mask_nii.get_fdata()\n",
    "\n",
    "        image_array = (image_array - np.mean(image_array)) / (np.std(image_array) + 1e-10)\n",
    "\n",
    "        phase_images, phase_masks = [], []\n",
    "        for i in range(image_array.shape[2]): \n",
    "            img_slice = image_array[:, :, i]\n",
    "            mask_slice = mask_array[:, :, i]\n",
    "            if img_slice.shape != (224, 224):\n",
    "                img_slice = resize_slice(img_slice, (224, 224))\n",
    "            if mask_slice.shape != (224, 224):\n",
    "                mask_slice = resize_slice(mask_slice, (224, 224))\n",
    "            img_slice = np.expand_dims(img_slice, axis=-1)\n",
    "            mask_slice = to_categorical(mask_slice, num_classes=num_classes).astype(\"float32\")\n",
    "            phase_images.append(img_slice)\n",
    "            phase_masks.append(mask_slice)\n",
    "        \n",
    "        patient_data.append({\n",
    "            'patient_id': patient_folder[-3:],\n",
    "            'phase': phase,\n",
    "            'images': np.array(phase_images, dtype=np.float32),\n",
    "            'masks': np.array(phase_masks, dtype=np.float32)\n",
    "        })\n",
    "\n",
    "    return patient_data\n",
    "\n",
    "def load_dataset_2d(root_dir, dataset_type):\n",
    "    dataset = []\n",
    "    dataset_path = os.path.join(root_dir, dataset_type)\n",
    "\n",
    "    for patient_folder in sorted(os.listdir(dataset_path)):\n",
    "        patient_path = os.path.join(dataset_path, patient_folder)\n",
    "        if os.path.isdir(patient_path):\n",
    "            patient_data = load_patient_slices(patient_path)\n",
    "            dataset.extend(patient_data)\n",
    "            print(f\"✅ Loaded {len(patient_data)} phases from {patient_folder}\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "data_2d = load_dataset_2d(r\"/home/student11/ds/mlmed-huyproman/cropped_slices_processed_320\", \"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 volumes from patient101\n",
      "✅ Loaded 2 volumes from patient102\n",
      "✅ Loaded 2 volumes from patient103\n",
      "✅ Loaded 2 volumes from patient104\n",
      "✅ Loaded 2 volumes from patient105\n",
      "✅ Loaded 2 volumes from patient106\n",
      "✅ Loaded 2 volumes from patient107\n",
      "✅ Loaded 2 volumes from patient108\n",
      "✅ Loaded 2 volumes from patient109\n",
      "✅ Loaded 2 volumes from patient110\n",
      "✅ Loaded 2 volumes from patient111\n",
      "✅ Loaded 2 volumes from patient112\n",
      "✅ Loaded 2 volumes from patient113\n",
      "✅ Loaded 2 volumes from patient114\n",
      "✅ Loaded 2 volumes from patient115\n",
      "✅ Loaded 2 volumes from patient116\n",
      "✅ Loaded 2 volumes from patient117\n",
      "✅ Loaded 2 volumes from patient118\n",
      "✅ Loaded 2 volumes from patient119\n",
      "✅ Loaded 2 volumes from patient120\n",
      "✅ Loaded 2 volumes from patient121\n",
      "✅ Loaded 2 volumes from patient122\n",
      "✅ Loaded 2 volumes from patient123\n",
      "✅ Loaded 2 volumes from patient124\n",
      "✅ Loaded 2 volumes from patient125\n",
      "✅ Loaded 2 volumes from patient126\n",
      "✅ Loaded 2 volumes from patient127\n",
      "✅ Loaded 2 volumes from patient128\n",
      "✅ Loaded 2 volumes from patient129\n",
      "✅ Loaded 2 volumes from patient130\n",
      "✅ Loaded 2 volumes from patient131\n",
      "✅ Loaded 2 volumes from patient132\n",
      "✅ Loaded 2 volumes from patient133\n",
      "✅ Loaded 2 volumes from patient134\n",
      "✅ Loaded 2 volumes from patient135\n",
      "✅ Loaded 2 volumes from patient136\n",
      "✅ Loaded 2 volumes from patient137\n",
      "✅ Loaded 2 volumes from patient138\n",
      "✅ Loaded 2 volumes from patient139\n",
      "✅ Loaded 2 volumes from patient140\n",
      "✅ Loaded 2 volumes from patient141\n",
      "✅ Loaded 2 volumes from patient142\n",
      "✅ Loaded 2 volumes from patient143\n",
      "✅ Loaded 2 volumes from patient144\n",
      "✅ Loaded 2 volumes from patient145\n",
      "✅ Loaded 2 volumes from patient146\n",
      "✅ Loaded 2 volumes from patient147\n",
      "✅ Loaded 2 volumes from patient148\n",
      "✅ Loaded 2 volumes from patient149\n",
      "✅ Loaded 2 volumes from patient150\n"
     ]
    }
   ],
   "source": [
    "def crop_or_pad_volume(volume, target_shape=(224, 224, 10)):\n",
    "    z, x, y = volume.shape\n",
    "    z_target, x_target, y_target = target_shape\n",
    "\n",
    "    if z < z_target:\n",
    "        pad_z = z_target - z\n",
    "        pad_before = pad_z // 2\n",
    "        pad_after = pad_z - pad_before\n",
    "        volume = np.pad(volume, ((pad_before, pad_after), (0, 0), (0, 0)), mode='constant')\n",
    "    elif z > z_target:\n",
    "        start_z = (z - z_target) // 2\n",
    "        volume = volume[start_z:start_z + z_target, :, :]\n",
    "    \n",
    "    if x < x_target or y < y_target:\n",
    "        volume = np.pad(volume, ((0, 0), (0, x_target - x), (0, y_target - y)), mode='constant')\n",
    "    elif x > x_target or y > y_target:\n",
    "        start_x = (x - x_target) // 2\n",
    "        start_y = (y - y_target) // 2\n",
    "        volume = volume[:, start_x:start_x + x_target, start_y:start_y + y_target]\n",
    "\n",
    "    return volume\n",
    "\n",
    "def load_patient_volume(patient_folder, num_classes=4):\n",
    "    patient_data = []\n",
    "\n",
    "    for phase in [\"ED\", \"ES\"]:\n",
    "        image_path = os.path.join(patient_folder, f\"patient{patient_folder[-3:]}_{phase}_processed.nii.gz\")\n",
    "        mask_path = os.path.join(patient_folder, f\"patient{patient_folder[-3:]}_{phase}_gt_processed.nii.gz\")\n",
    "\n",
    "        if not os.path.exists(image_path) or not os.path.exists(mask_path):\n",
    "            print(f\"⚠️ Missing file: {image_path} or {mask_path}\")\n",
    "            continue  \n",
    "\n",
    "        image_nii = nib.load(image_path)\n",
    "        mask_nii = nib.load(mask_path)\n",
    "\n",
    "        image_array = image_nii.get_fdata()\n",
    "        mask_array = mask_nii.get_fdata()\n",
    "\n",
    "        original_z = image_array.shape[2]  # Store original number of slices\n",
    "        image_array = (image_array - np.mean(image_array)) / (np.std(image_array) + 1e-10)\n",
    "\n",
    "        image_array = crop_or_pad_volume(image_array)\n",
    "        mask_array = crop_or_pad_volume(mask_array)\n",
    "\n",
    "        image_array = np.expand_dims(image_array, axis=-1)\n",
    "        mask_array = to_categorical(mask_array, num_classes=num_classes).astype(\"float32\")\n",
    "\n",
    "        patient_data.append({\n",
    "            'patient_id': patient_folder[-3:],\n",
    "            'phase': phase,\n",
    "            'image': image_array,\n",
    "            'mask': mask_array,\n",
    "            'original_z': original_z\n",
    "        })\n",
    "\n",
    "    return patient_data\n",
    "\n",
    "def load_dataset_3d(root_dir, dataset_type):\n",
    "    dataset = []\n",
    "    dataset_path = os.path.join(root_dir, dataset_type)\n",
    "\n",
    "    for patient_folder in sorted(os.listdir(dataset_path)):\n",
    "        patient_path = os.path.join(dataset_path, patient_folder)\n",
    "        if os.path.isdir(patient_path):\n",
    "            patient_data = load_patient_volume(patient_path)\n",
    "            dataset.extend(patient_data)\n",
    "            print(f\"✅ Loaded {len(patient_data)} volumes from {patient_folder}\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "data_3d = load_dataset_3d(r\"/home/student11/ds/mlmed-huyproman/cropped_slices_processed_320\", \"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All good. No mismatches.\n"
     ]
    }
   ],
   "source": [
    "mismatches = []\n",
    "\n",
    "for d2, d3 in zip(data_2d, data_3d):\n",
    "    slices_2d = d2['images'].shape[0]\n",
    "    slices_3d = d3['original_z']\n",
    "    if slices_2d != slices_3d:\n",
    "        mismatches.append((d2['patient_id'], d2['phase'], slices_2d, slices_3d))\n",
    "\n",
    "if mismatches:\n",
    "    print(\"❌ Slice count mismatches found:\")\n",
    "    for patient_id, phase, slices_2d, slices_3d in mismatches:\n",
    "        print(f\"Patient {patient_id}, Phase {phase}: 2D = {slices_2d}, 3D = {slices_3d}\")\n",
    "else:\n",
    "    print(\"✅ All good. No mismatches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 18:53:37.371307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6653 MB memory:  -> device: 0, name: Quadro RTX 4000, pci bus id: 0000:12:00.0, compute capability: 7.5\n",
      "2025-04-02 18:53:37.372566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 6653 MB memory:  -> device: 1, name: Quadro RTX 4000, pci bus id: 0000:86:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "def create_tf_dataset_2d(patient_data, batch_size):\n",
    "    images = np.concatenate([p['images'] for p in patient_data], axis=0)\n",
    "    masks = np.concatenate([p['masks'] for p in patient_data], axis=0)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, masks))\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def create_tf_dataset_3d(patient_data, batch_size):\n",
    "    images = np.array([p['image'] for p in patient_data], dtype=np.float32)\n",
    "    masks = np.array([p['mask'] for p in patient_data], dtype=np.float32)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, masks))\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    test_dataset_2d = create_tf_dataset_2d(data_2d, batch_size=1)\n",
    "    test_dataset_3d = create_tf_dataset_3d(data_3d, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss_per_class(y_true, y_pred, num_classes=4, smooth=1e-6):\n",
    "    dice_scores = []\n",
    "    \n",
    "    for i in range(1, num_classes):\n",
    "        y_true_class = K.flatten(y_true[..., i])\n",
    "        y_pred_class = K.flatten(y_pred[..., i])\n",
    "\n",
    "        intersection = K.sum(y_true_class * y_pred_class)\n",
    "        dice = (2. * intersection + smooth) / (K.sum(y_true_class) + K.sum(y_pred_class) + smooth)\n",
    "        dice_scores.append(dice)\n",
    "\n",
    "    return 1 - K.mean(K.stack(dice_scores), axis=0)  \n",
    "\n",
    "def dice_score_per_class(y_true, y_pred, num_classes=4, smooth=1e-6):\n",
    "    dice_scores = []\n",
    "    \n",
    "    for i in range(1, num_classes):\n",
    "        y_true_class = K.flatten(y_true[..., i])\n",
    "        y_pred_class = K.flatten(y_pred[..., i])\n",
    "\n",
    "        intersection = K.sum(y_true_class * y_pred_class)\n",
    "        dice = (2. * intersection + smooth) / (K.sum(y_true_class) + K.sum(y_pred_class) + smooth)\n",
    "        dice_scores.append(dice)\n",
    "\n",
    "    return K.mean(K.stack(dice_scores), axis=0)  \n",
    "\n",
    "def dice_metric(y_true, y_pred):\n",
    "    return dice_score_per_class(y_true, y_pred, num_classes=4)\n",
    "\n",
    "def dice_score_each_individual_class(y_true, y_pred, num_classes=4, smooth=1e-6):\n",
    "    dice_scores = []\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        y_true_class = tf.keras.backend.flatten(y_true[..., i])\n",
    "        y_pred_class = tf.keras.backend.flatten(y_pred[..., i])\n",
    "\n",
    "        intersection = tf.keras.backend.sum(y_true_class * y_pred_class)\n",
    "        dice = (2. * intersection + smooth) / (\n",
    "            tf.keras.backend.sum(y_true_class) + tf.keras.backend.sum(y_pred_class) + smooth\n",
    "        )\n",
    "        dice_scores.append(dice)\n",
    "\n",
    "    return dice_scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss_per_class_3d(y_true, y_pred, num_classes=4, smooth=1e-6):\n",
    "    dice_scores = []\n",
    "    for i in range(1, num_classes):\n",
    "        y_true_class = K.flatten(y_true[..., i])  \n",
    "        y_pred_class = K.flatten(y_pred[..., i])\n",
    "        intersection = K.sum(y_true_class * y_pred_class)\n",
    "        dice = (2. * intersection + smooth) / (K.sum(y_true_class) + K.sum(y_pred_class) + smooth)\n",
    "        dice_scores.append(dice)\n",
    "    mean_dice = K.mean(K.stack(dice_scores), axis=0)\n",
    "    return 1 - mean_dice\n",
    "def dice_score_per_class_3d(y_true, y_pred, num_classes=4, smooth=1e-6):\n",
    "    dice_scores = []\n",
    "    for i in range(1, num_classes):  \n",
    "        y_true_class = K.flatten(y_true[..., i])\n",
    "        y_pred_class = K.flatten(y_pred[..., i])\n",
    "        intersection = K.sum(y_true_class * y_pred_class)\n",
    "        dice = (2. * intersection + smooth) / (K.sum(y_true_class) + K.sum(y_pred_class) + smooth)\n",
    "        dice_scores.append(dice)\n",
    "    return K.mean(K.stack(dice_scores), axis=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2d = tf.keras.models.load_model(r\"/home/student11/ds/mlmed-huyproman/models/best_unet2D_320_again_1\",\n",
    "                                      custom_objects={\n",
    "                                    'dice_loss_per_class': dice_loss_per_class,\n",
    "                                    'dice_metric': dice_metric\n",
    "                                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3d = tf.keras.models.load_model(r\"/home/student11/ds/mlmed-huyproman/models/best_unet3D_224x224x10\",\n",
    "                                      custom_objects={\n",
    "                                    'dice_loss_per_class_3d': dice_loss_per_class_3d,\n",
    "                                    'dice_score_per_class_3d': dice_score_per_class_3d\n",
    "                                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 18:53:46.443688: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2025-04-02 18:53:46.999380: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "def get_predictions(model, dataset):\n",
    "    predictions = []\n",
    "    for x, _ in dataset:\n",
    "        pred = model.predict(x, verbose=0)\n",
    "        if isinstance(pred, dict):\n",
    "            pred = list(pred.values())[0]\n",
    "        predictions.append(pred)\n",
    "    return tf.concat(predictions, axis=0)\n",
    "\n",
    "y_pred_2d = get_predictions(model_2d, test_dataset_2d)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 18:54:41.012088: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.97GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    y_pred_3d = []\n",
    "    for batch in test_dataset_3d:\n",
    "        images, _ = batch\n",
    "        pred = model_3d.predict(images, verbose=0)\n",
    "        main_pred = pred[0]  \n",
    "        y_pred_3d.append(main_pred)\n",
    "\n",
    "y_pred_3d = tf.concat(y_pred_3d, axis=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_2d = []\n",
    "for _, y in test_dataset_2d:\n",
    "    y_true_2d.append(y)\n",
    "y_true_2d = tf.concat(y_true_2d, axis=0)\n",
    "\n",
    "# y_true_3d = []\n",
    "# for batch in test_dataset_3d:\n",
    "#     _, masks = batch  \n",
    "#     y_true_3d.append(masks[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_preds = []\n",
    "\n",
    "for i, patient in enumerate(data_3d):\n",
    "    image = patient[\"image\"]  \n",
    "    pred = y_pred_3d[i]    \n",
    "\n",
    "    for z in range(image.shape[2]):\n",
    "        img_slice = image[:, :, z, 0]\n",
    "        if not tf.reduce_all(img_slice == 0):\n",
    "            pred_slice = pred[:, :, z, :]\n",
    "            filtered_preds.append(pred_slice)\n",
    "\n",
    "y_pred_3d_flat = tf.stack(filtered_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_predictions = []\n",
    "current_3d_idx = 0\n",
    "\n",
    "for vol_data in data_3d:\n",
    "    original_z = vol_data['original_z']\n",
    "    y_pred_2d_valid = y_pred_2d[current_3d_idx : current_3d_idx + original_z]\n",
    "    y_pred_3d_valid = y_pred_3d_flat[current_3d_idx : current_3d_idx + original_z]\n",
    "    fused_pred = y_pred_2d_valid * 0.6 + y_pred_3d_valid * 0.4\n",
    "    fused_predictions.append(fused_pred)\n",
    "    current_3d_idx += original_z\n",
    "\n",
    "fused_predictions = tf.concat(fused_predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice score for class 1: 90.85%\n",
      "Dice score for class 2: 87.91%\n",
      "Dice score for class 3: 95.24%\n",
      "Average Dice score for main classes: 91.57%\n"
     ]
    }
   ],
   "source": [
    "y_pred_labels = tf.argmax(fused_predictions, axis=-1)  \n",
    "y_true_labels = tf.argmax(y_true_2d, axis=-1)  \n",
    "\n",
    "def dice_score_per_class(y_true, y_pred, num_classes=4, smooth=1e-6):\n",
    "    dice_scores = []\n",
    "    for c in range(1, num_classes):\n",
    "        y_true_c = tf.cast(y_true == c, tf.float32)\n",
    "        y_pred_c = tf.cast(y_pred == c, tf.float32)\n",
    "        intersection = tf.reduce_sum(y_true_c * y_pred_c)\n",
    "        union = tf.reduce_sum(y_true_c) + tf.reduce_sum(y_pred_c)\n",
    "        dice = (2. * intersection + smooth) / (union + smooth)\n",
    "        dice_scores.append(dice)\n",
    "    return dice_scores\n",
    "\n",
    "with strategy.scope():\n",
    "    dice_scores = dice_score_per_class(y_true_labels, y_pred_labels)\n",
    "\n",
    "    for c, score in enumerate(dice_scores):\n",
    "        print(f\"Dice score for class {c+1}: {score.numpy()*100:.2f}%\")\n",
    "\n",
    "    average_main = tf.reduce_mean(dice_scores[1:4])\n",
    "    print(f\"Average Dice score for main classes: {average_main.numpy()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fucking dick urkel piece of shit cái trên này fix mãi đ đc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# FINAL RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.6 - 0.4\n",
    "* Dice score for class 1: 90.85%\n",
    "\n",
    "* Dice score for class 2: 87.91%\n",
    "\n",
    "* Dice score for class 3: 95.24%\n",
    "\n",
    "=>Average Dice score for main classes: 91.57%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlmed-huyproman",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
